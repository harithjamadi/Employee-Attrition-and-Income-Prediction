---
title: "Dual-Model Approach to HR Analytics: Employee Attrition and Income Prediction"
author: Harith/ Firdaus/ Hanisah
output: html_document
date: "2025-06-15"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos = c(CRAN = "https://cloud.r-project.org"))
```
## Introduction

An HR Management Development Program is often structured to provide a thorough blend of theoretical knowledge and practical experience, ensuring that participants gain well-rounded competencies. The program is frequently separated into main modules that address essential HR responsibilities like as recruitment, talent management, employee engagement, and labor law compliance

## Objectives

1. To explore patterns in employee behavior.
2. To predict whether an employee is likely to leave the company (attrition) and to estimate their monthly income.
3. To evaluate and compare between machine learning models for the best-performing model.

## Data preparation

```{r load-packages}
# Install required packages if missing
packages <- c("skimr", "DataExplorer", "corrplot", "ggplot2", "dplyr", "cluster", "factoextra", "caret", "randomForest", "gbm")
installed <- packages %in% installed.packages()
if (any(!installed)) install.packages(packages[!installed])

# Load packages
library(skimr)
library(DataExplorer)
library(corrplot)
library(ggplot2)
library(dplyr)
library(cluster)
library(factoextra)
library(caret)
library(randomForest)
library(gbm)
library(reshape2)

set.seed(123)

# Load data
attrition <- read.csv("https://raw.githubusercontent.com/harithjamadi/Employee-Attrition-and-Income-Prediction/main/HR.csv")
head(attrition)
```

## Data Cleaning, Understanding & Exploration

```{r data-wrangling}
# Check structure and summary
# → Understand data types, detect categorical/numeric variables, and get an overview of distributions.
str(attrition)

summary(attrition)

skim(attrition)

```

```{r data-cleaning}
# Check for missing values and duplicates
# → Ensure data completeness and integrity before analysis.
colSums(is.na(attrition))

# Visualize missing data
plot_missing(attrition)

# Check for duplicate rows
sum(duplicated(attrition))
```

```{r data-encoding}
# Convert categorical variables to factors
# → Required for correct handling in models and visualizations.
attrition <- attrition %>%
  mutate(
    Attrition = as.factor(Attrition),
    BusinessTravel = as.factor(BusinessTravel),
    Department = as.factor(Department),
    EducationField = as.factor(EducationField),
    Gender = as.factor(Gender),
    JobRole = as.factor(JobRole),
    MaritalStatus = as.factor(MaritalStatus),
    OverTime = as.factor(OverTime),
    Over18 = as.factor(Over18)
  )

str(attrition) # Re-check structure after conversion
```

## Summary Statistics Grouped by Attrition
```{r summary-statistics}
# Generate summary statistics grouped by Attrition
# → Compare means and medians of numeric features between employees who left and stayed.
attrition %>%
  group_by(Attrition) %>%
  summarise(across(where(is.numeric), list(mean = mean, median = median), .names = "{.col}_{.fn}"))
```

## Attrition Distribution
```{r attrition-distribution}
# Visualize overall attrition distribution
# → Understand class imbalance in the target variable.
ggplot(attrition, aes(x = Attrition, fill = Attrition)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Attrition Distribution", y = "Count")
```
## Correlation Analysis (Numerical)
```{r correlation-analysis}
# Analyze correlations among numeric variables
# → Identify multicollinearity and spot influential numeric predictors.
numeric_vars <- attrition %>% select(where(is.numeric))

# Correlation matrix
cor_matrix <- cor(numeric_vars)

# Visualize correlation
corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.8)
```

## Univariate & Bivariate Analysis
```{r variate-analysis}
# Visualize distributions and relationships between features and Attrition
# → Explore potential predictors and patterns.

# Age distribution (Histogram)
# → Understand overall age trends. Histogram is used to show frequency distribution of continuous variable (Age).
ggplot(attrition, aes(x = Age)) +
  geom_histogram(fill = "steelblue", bins = 30) +
  theme_minimal() +
  labs(title = "Distribution of Age")

# Monthly income by Attrition (Boxplot)
# → Compare income distribution between employees who stayed vs left. Boxplot shows medians and outliers clearly.
ggplot(attrition, aes(x = Attrition, y = MonthlyIncome, fill = Attrition)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Monthly Income by Attrition")

# Job Role vs Attrition (Proportional Bar Chart)
# → Identify which job roles have higher attrition. Proportional bar chart (position = "fill") is ideal for comparing proportions within categories.
ggplot(attrition, aes(x = JobRole, fill = Attrition)) +
  geom_bar(position = "fill") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Attrition Rate by Job Role", y = "Proportion")

# Overtime vs Attrition (Proportional Bar Chart)
# → Analyze how working overtime affects attrition. Proportional bar chart is used to compare ratios across groups.
ggplot(attrition, aes(x = OverTime, fill = Attrition)) +
  geom_bar(position = "fill") +
  theme_minimal() +
  labs(title = "Attrition Rate by Overtime", y = "Proportion")

# Gender vs Attrition (Proportional Bar Chart)
# → See if attrition varies by gender. Bar chart with proportion helps normalize different group sizes.
ggplot(attrition, aes(x = Gender, fill = Attrition)) +
  geom_bar(position = "fill") +
  theme_minimal() +
  labs(title = "Attrition Rate by Gender", y = "Proportion")

# Marital Status vs Attrition (Proportional Bar Chart)
# → Assess marital status influence on attrition. Again, proportion bar chart allows fair comparison across groups.
ggplot(attrition, aes(x = MaritalStatus, fill = Attrition)) +
  geom_bar(position = "fill") +
  theme_minimal() +
  labs(title = "Attrition Rate by Marital Status", y = "Proportion")

# Age by Attrition (Boxplot)
# → Compare age ranges across attrition groups. Boxplot efficiently shows medians, spread, and outliers.
ggplot(attrition, aes(x = Attrition, y = Age, fill = Attrition)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Age Distribution by Attrition")

# Years at Company by Attrition (Histogram with overlay)
# → Examine tenure patterns among employees. Histogram shows frequency, with transparency (alpha) to allow group comparison.
ggplot(attrition, aes(x = YearsAtCompany, fill = Attrition)) +
  geom_histogram(position = "identity", alpha = 0.6, bins = 20) +
  theme_minimal() +
  labs(title = "Years at Company by Attrition")

# Job Satisfaction vs Attrition (Stacked Bar Chart)
# → Explore satisfaction levels related to attrition. Categorical x-axis, so bar chart is suitable. Proportions show attrition rate by rating.
ggplot(attrition, aes(x = factor(JobSatisfaction), fill = Attrition)) +
  geom_bar(position = "fill") +
  theme_minimal() +
  labs(title = "Attrition by Job Satisfaction", x = "Job Satisfaction (1â€“4)", y = "Proportion")

# Business Travel vs Attrition (Proportional Bar Chart)
# → Determine if frequent travel affects attrition. Bar chart visualizes proportion of leavers in each travel group.
ggplot(attrition, aes(x = BusinessTravel, fill = Attrition)) +
  geom_bar(position = "fill") +
  theme_minimal() +
  labs(title = "Attrition Rate by Business Travel", y = "Proportion")

# Total Working Years vs Monthly Income by Attrition (Scatter Plot)
# → Investigate relationship between experience and income, and how it differs for attrition status. Scatter plot best for continuous vs continuous.
ggplot(attrition, aes(x = TotalWorkingYears, y = MonthlyIncome, color = Attrition)) +
  geom_point(alpha = 0.6) +
  theme_minimal() +
  labs(title = "Total Working Years vs. Monthly Income by Attrition")

```

## Data Preprocessing
This section prepares dataset for both regression and clustering predictive modelling, as it improve data quality and model performance.
The process start off by dropping non-informative columns such as EmployeeCount, EmployeeNumber, Over18, and StandardHours to minimizes the noise and redundancy.
Then, all numerical features being scaled to ensure comparability across different scales, making it more sensitive to magnitude.
Lastly, transforming existing categorical features into numeric format using one-hot encoding from caret package.
The scaled data then being split into training set (80%) and testing set (20%) for model training and evaluation purpose.

```{r data-preprocessing}
# Remove non-information columns
attrition <- subset(attrition, select = -c(EmployeeCount, EmployeeNumber, Over18, StandardHours))

# Scaling numerical features
numeric_cols <- sapply(attrition, is.numeric)
hr_scaled <- attrition
hr_scaled[numeric_cols] <- scale(attrition[numeric_cols])

# Encode categorical features
dummies <- dummyVars(~ ., data = attrition)
hr_dummy <- predict(dummies, newdata = attrition)
hr_scaled <- as.data.frame(scale(hr_dummy))

# Train-test split
split <- createDataPartition(hr_scaled$MonthlyIncome, p = 0.8, list = FALSE)
train <- hr_scaled[split, ]
test <- hr_scaled[-split, ]

```

## Predictive Modeling - Regression Analysis
There are three regression model that being used to predict employees' monthly income based on the HR-related data. This includes :-

- Linear Regression - a parametric model that estimates a linear relationship between dependent variable and one or more independent variables.
- Random Forest - an ensemble method that builds multiple decision trees and combines their outputs to improve predictive accuracy and control overfitting
- Gradient Boosting - an ensemble method that builds decision tree and aims to correct the errors of the previous ones.

### Linear Regression Model
Linear regression model is trained on scaled training data to serve as a baseline. If often assumes there is linear relationship between predictors and target variable (MonthlyIncome). It being used because of the simplicity and provides as benchmark for different model performances.
```{r lm-model}
model_lm <- train(MonthlyIncome ~ ., data = train, method = "lm")
```

### Random Forest Model
Random Forest Regression is a robust model that capable of constructing multiple decision trees and average their predictions. We trained the model with 5-fold cross-validation to ensure that it will perform well on unseen data. Random forest also excels in capturing nonlinear and complex relationship between data.
```{r rf-model}
model_rf <- train(MonthlyIncome ~ ., data = train, method = "rf", trControl = trainControl(method = "cv", number = 5))
```

### Gradient Boosting Model
Gradient Boosting Model share the same robustness like Random Forest but with addition of self-correcting features. Each generated tree corrects the error made by the previous ensemble, improving it accuracy over time. Despite that, it can be more sensitive to noise and overfitting which is something that we usually want to avoid.
```{r gbm-model}
model_gbm <- train(MonthlyIncome ~ ., data = train, method = "gbm", verbose = FALSE, trControl = trainControl(method = "cv", number = 5))
```

### Model Evaluation
For the predictive performance, it will be evaluated through Root Mean Squared Error (RMSE), R^2 (coefficient of determination) and Mean Absolute Error (MAE) on test dataset. In this three model, Random Forest seems to outbest other model with highest R^2 and lowest RMSE and MAE, showing the superior ability to accurately predict the MonthlyIncome. Followed closely by Gradient Boosting, while Linear Regression performs relatively worse. 
```{r model-evaluation}
pred_lm <- predict(model_lm, newdata = test)
pred_rf <- predict(model_rf, newdata = test)
pred_gbm <- predict(model_gbm, newdata = test)

postResample(pred_lm, obs = test$MonthlyIncome)
postResample(pred_rf, obs = test$MonthlyIncome)
postResample(pred_gbm, obs = test$MonthlyIncome)
```

### Prediction Visualization
A Scatter Plot to compares the actual vs predicted monthly incomes for the Random Forest model. The dashed red line indicates the perfect case where predicted values match the actual incomes. The scatter plot helps to highlight how well the model generalizes, and identifies pattern in dataset.
```{r model-prediction}
ggplot(data.frame(Actual = test$MonthlyIncome, Predicted = pred_rf), aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_abline(color = "red", linetype = "dashed") +
  theme_minimal() +
  labs(title = "Predicted vs Actual Monthly Income", x = "Actual", y = "Predicted")
```

### Feature Importance
Feature Importance is extracted to identify which variables influence prediction of monthly income. JobLevel appears as the most influential predictor, showing its strong relationship with salary bands. Other remarkable features include TotalWorkingYears and JobRole, which also contribute significantly to predict the monthly income.
```{r feature-ranking}
varImp(model_rf)
```

### Pay Equity Analysis
This analysis investigates model performance to under- or over-predicts income for certain employee. By calculating the difference between predicted and actual incomes, it is possible to identify top underpaid or overpaid individuals based on the model output. These insights could be utilized by audit to compensate fairness and support HR decision-making. However, there might be prediction error by product of imbalance and model bias, especially for outlier values.
```{r pay-equity-analysis}
test_results <- data.frame(Actual = test$MonthlyIncome, Predicted = pred_rf)
test_results$Difference <- test_results$Predicted - test_results$Actual

# Show top 10 underpaid employees
head(arrange(test_results, Difference), 10)
```

## Clustering Analysis

This clustering analysis aims to uncover distinct patterns and groupings within the HR dataset. Since the dataset contains multiple continuous variables without predefined labels, clustering provides an effective way to segment observations into meaningful groups based on their similarities.
To ensure robust and reliable findings, three different clustering algorithms are applied and compared:

- K-Means Clustering – a widely used, centroid-based method that partitions data into clusters with minimal within-cluster variance.
- Hierarchical Clustering (Ward’s Method) – a tree-based approach that does not require pre-specifying the number of clusters and helps visualize nested structures in the data.
- Partitioning Around Medoids (PAM) – a medoid-based alternative to K-Means that is more resilient to outliers and non-spherical clusters.

### K-Means Clustering
K-Means is a fast and widely used clustering method that partitions data into k clusters by minimizing within-cluster variance. In this analysis, we removed any pre-existing cluster labels to avoid contamination, applied K-Means with 3 clusters and 25 random starts for stability, and added the resulting labels back to the dataset for profiling. The clusters were then visualized using the first two principal components to aid interpretation.
```{r K-Means Clustering}
# Copy the scaled data to avoid issues
hr_input <- hr_scaled[, !names(hr_scaled) %in% "Cluster"]

# Run k-means on clean numeric data
kmeans_result <- kmeans(hr_input, centers = 3, nstart = 25)

# Add cluster labels to the original data
hr_scaled$Cluster <- as.factor(kmeans_result$cluster)

# Visualize
fviz_cluster(kmeans_result, data = hr_input, geom = "point", ellipse.type = "convex",
             palette = "jco", ggtheme = theme_minimal())
```

### Hierarchical Clustering
Hierarchical clustering builds a tree-like structure of nested clusters using Ward’s linkage method, which minimizes within-cluster variance. After computing the distance matrix, a dendrogram is plotted to visualize the grouping structure. The tree is then cut to form 3 clusters, which are assigned to the data and visualized for interpretation.
```{r Hierarchical Clustering}
# Compute distance and linkage
dist_matrix <- dist(hr_input)
hc_result <- hclust(dist_matrix, method = "ward.D2")

# Dendrogram
plot(hc_result, labels = FALSE, hang = -1, main = "Hierarchical Clustering Dendrogram")

# Cut into 3 clusters
hr_hclust <- hr_scaled
hr_hclust$Cluster <- as.factor(cutree(hc_result, k = 3))

# Visualize
fviz_cluster(list(data = hr_input, cluster = hr_hclust$Cluster),
             geom = "point", ellipse.type = "convex", palette = "jco", ggtheme = theme_minimal())

```

### Partitioning Around Medoids (PAM) Clustering
PAM (Partitioning Around Medoids) is a robust clustering method that, unlike K-Means, uses medoids, making it less sensitive to outliers and better suited for datasets with noise or non-spherical shapes. In this analysis, PAM was applied with 3 clusters, and the resulting labels were added to the dataset and visualized for interpretation.
```{r PAM (Partitioning Around Medoids)}

pam_result <- pam(hr_scaled, k = 3)
hr_pam <- hr_scaled
hr_pam$Cluster <- as.factor(pam_result$clustering)

fviz_cluster(pam_result, geom = "point", ellipse.type = "convex",
             palette = "jco", ggtheme = theme_minimal())

```

### Cluster Profiling
Based on visual comparison, we selected K-Means Clusters as the most interpretable clustering solution due to its clear separation and minimal overlap between clusters. 
Cluster profiling was then performed by computing feature means within each cluster and identifying the top 30 features with the highest variance across groups. 
These key differentiators were visualized using bar plots to clearly highlight distinctions between clusters.

```{r Cluster Profiling & visualisation}
# KMeans
kmeans_summary <- aggregate(hr_scaled[, -which(names(hr_scaled) == "Cluster")],
                            by = list(Cluster = hr_scaled$Cluster), FUN = mean)

# Compute variance of mean feature values across clusters
feature_variances <- apply(kmeans_summary[,-1], 2, var)

# Select top 30 (10/10/10) features
features1 <- names(sort(feature_variances, decreasing = TRUE))[1:10]
features2 <- names(sort(feature_variances, decreasing = TRUE))[11:20]
features3 <- names(sort(feature_variances, decreasing = TRUE))[21:30]

melted_summary <- melt(kmeans_summary, id.vars = "Cluster")


melted1 <- melted_summary[melted_summary$variable %in% features1, ]

ggplot(melted1, aes(x = variable, y = value, fill = Cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_minimal() +
  labs(title = "First 10 Discriminative Features by Cluster",
       x = "Feature", y = "Mean (scaled)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


melted2 <- melted_summary[melted_summary$variable %in% features2, ]

ggplot(melted2, aes(x = variable, y = value, fill = Cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_minimal() +
  labs(title = "Second 10 Discriminative Features by Cluster",
       x = "Feature", y = "Mean (scaled)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

melted3 <- melted_summary[melted_summary$variable %in% features3, ]

ggplot(melted3, aes(x = variable, y = value, fill = Cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_minimal() +
  labs(title = "Third 10 Discriminative Features by Cluster",
       x = "Feature", y = "Mean (scaled)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
### Key Findings

**Cluster 1 (Red)**
Likely early-career employees in Sales or Research roles with lower tenure and income, potentially at risk of attrition.

- Department: Mostly in Sales.
- Roles: Sales Rep, Research Scientist.
- Experience: Low job level, income, and tenure.
- Attrition: Higher risk of leaving.

**Cluster 2 (Green)**
Likely mid-level sales/marketing professionals with moderate experience and lower attrition rates.

- Department: Strong in Sales and Marketing.
- Roles: Sales Exec, Healthcare Rep.
- Experience: Moderate income and tenure.
- Attrition: Lower risk of leaving.

**Cluster 3 (Blue)**
Senior professionals/managers, highly experienced, with strong tenure and income, and least likely to leave.

- Department: Mainly in R&D.
- Roles: Managers, Directors.
- Experience: High job level, income, and years at company.
- Attrition: Very low risk.

The clustering results reveal that tenure and income are key factors distinguishing employee groups. Attrition likelihood is strongly linked to both experience level and departmental placement, with job roles reflecting the employees' career stages. Overall, Cluster 3 consists of the most experienced and stable employees, while Cluster 1 is characterized by junior staff who may be more at risk of leaving the organization.